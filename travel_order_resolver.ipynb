{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HjSBTZQlHut"
      },
      "source": [
        "## **Objectif:**\n",
        "#### Le but de ce projet est de proposer à un utilisateur le chemin optimal pour rejoindre une destination en France.\n",
        "\n",
        "## **Fonctionnalités:**\n",
        "*   L'utilisateur enregistre un audio avec une phrase du genre: ***Je suis chez moi à Paris et je veux me rendre à Dijon pour une remise de diplôme***.\n",
        "\n",
        "*   Le programme extrait la ville de départ et d'arrivée puis propose le plus court chemin pour arriver à destination.\n",
        "\n",
        "\n",
        "## **Techniques utilisées:**\n",
        "*   NLP (Natural Language Processing)\n",
        "*   Reconnaissance Vocale\n",
        "*   Algorithmique de parcours de graphes\n",
        "\n",
        "\n",
        "## **Jeux de données:**\n",
        "*   Nous avons créé un jeu de données qui contient des exemples de phrases qui vont aider à entrainer notre modèle pour reconnaitre les villes de départ et d'arrivée dans la demande de l'utilisateur. https://drive.google.com/uc?export=download&id=1n8vKheaOfZfekyUn94ax40RB3cU0BS5P\n",
        "\n",
        "*   Nous utilisons également un jeu de données de la SNCF qui affiche les durées de trajet en fonction de la ville de départ et la ville d'arrivée. Ce jeu de données va nous permettre de determiner le plus court chemin entre une ville de départ et une ville d'arrivée. https://drive.google.com/uc?export=download&id=1CL_z0bH464x42fRfWwM1qJaGKjd9fCtp.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTfi1hf0oC2A"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc-ffkXYHbqF"
      },
      "source": [
        "### Module 1: Speech to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgrqtzwHlMax",
        "outputId": "ffafb4e4-9464-4670-8a35-804d3998bafa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!pip install ffmpeg-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxU1TH6Vn54m"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "from scipy.io.wavfile import write\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };\n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  byte_io = io.BytesIO(bytes())\n",
        "  write(byte_io, sr, audio)\n",
        "  result_bytes = byte_io.read()\n",
        "\n",
        "  audio_data = speech_recognition.AudioData(result_bytes, sr, 2)\n",
        "\n",
        "  return audio_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_qwDgUWeyLR",
        "outputId": "a5786e3f-4827-4747-9525-4a9dad929563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting speechRecognition\n",
            "  Downloading SpeechRecognition-3.10.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from speechRecognition) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechRecognition) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechRecognition) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechRecognition) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechRecognition) (3.4)\n",
            "Installing collected packages: speechRecognition\n",
            "Successfully installed speechRecognition-3.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install speechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition"
      ],
      "metadata": {
        "id": "2SqJoc1KdSYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI54Pv828GhW"
      },
      "outputs": [],
      "source": [
        "def speech_to_text(audio_data):\n",
        "\n",
        "  r = speech_recognition.Recognizer()\n",
        "\n",
        "  try:\n",
        "    output = r.recognize_google(audio_data, language='fr-FR', show_all=True)\n",
        "    travel_request = output[\"alternative\"][0][\"transcript\"]\n",
        "    print(\"The travel request sentence is : \", travel_request)\n",
        "    return travel_request\n",
        "  except speech_recognition.RequestError as e:\n",
        "      print(\"Could not request results; {0}\".format(e))\n",
        "\n",
        "  except speech_recognition.UnknownValueError as e:\n",
        "      print(\"unknown error occured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvvSFN7WbO8d"
      },
      "source": [
        "## Module 2: NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzMj2il1oobh"
      },
      "source": [
        "### 3.0 Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbpJGaXFonQu",
        "outputId": "a6f60fe1-c854-48b3-e310-c3f85396ef6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jeu de test : 90\n",
            "                                            sentence    departure  destination\n",
            "0  Je voudrais aller à Paris en partant de Montpe...  Montpellier        Paris\n",
            "1  Est-ce que tu peux me trouver un itinéraire de...        Paris  Montpellier\n",
            "2  Je souhaite aller à Lyon en partant de Montpel...  Montpellier         Lyon\n",
            "3        Je suis à Montpellier, je vous aller à Lyon  Montpellier         Lyon\n",
            "4           Trouve moi un itinéraire pour Lyon Paris         Lyon        Paris\n",
            "sentence       object\n",
            "departure      object\n",
            "destination    object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "urllib.request.urlretrieve(\"https://drive.google.com/uc?export=download&id=1n8vKheaOfZfekyUn94ax40RB3cU0BS5P\", \"sentence_dataset.xlsx\")\n",
        "\n",
        "excel_data = pd.read_excel(\"/content/sentence_dataset.xlsx\", sheet_name=\"Test\")\n",
        "print(f'Jeu de test : {len(excel_data)}')\n",
        "print(excel_data[0:5])\n",
        "print(excel_data.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__rHHQjbtTly",
        "outputId": "e638635b-70df-4f5c-b232-5037f3b6861f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-06 11:51:36.993063: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-06 11:51:38.103115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-06 11:51:39.595367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-06 11:51:39.595923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-06 11:51:39.596118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting fr-core-news-md==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.5.0/fr_core_news_md-3.5.0-py3-none-any.whl (45.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-md==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->fr-core-news-md==3.5.0) (2.1.3)\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_md')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download fr_core_news_md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSo8D90Dbeod"
      },
      "source": [
        "### 3.1 Algorithmic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d_wAp90bcrd"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_md\")\n",
        "\n",
        "def location_count(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    locations = tuple(filter(lambda x: (x.label_ == \"LOC\"),doc.ents))\n",
        "    return True if len(locations) == 2 else False\n",
        "\n",
        "def predict(d):\n",
        "\n",
        "    departure = \"\"\n",
        "    arrival = \"\"\n",
        "\n",
        "    children_0_strings = [i.text for i in d[list(d.keys())[0]][\"children\"]]\n",
        "    children_1_strings = [i.text for i in d[list(d.keys())[1]][\"children\"]]\n",
        "\n",
        "    head_0_string = d[list(d.keys())[0]][\"head\"]\n",
        "    head_1_string = d[list(d.keys())[1]][\"head\"]\n",
        "\n",
        "    arrival_children = [\"à\", \"pour\"]\n",
        "    departure_children = [\"de\", \"depuis\"]\n",
        "\n",
        "    arrival_head = [\"aller\"]\n",
        "    departure_head = [\"suis\", \"partant\"]\n",
        "\n",
        "    if any(i in departure_children for i in children_0_strings) and not any(i in departure_children for i in children_1_strings) :\n",
        "        departure = list(d.keys())[0]\n",
        "        arrival = list(d.keys())[1]\n",
        "    elif any(i in departure_children for i in children_1_strings) and not any(i in departure_children for i in children_0_strings) :\n",
        "        departure = list(d.keys())[1]\n",
        "        arrival = list(d.keys())[0]\n",
        "    elif any(i in departure_head for i in head_0_string) and not any(i in departure_head for i in head_1_string) :\n",
        "        departure = list(d.keys())[0]\n",
        "        arrival = list(d.keys())[1]\n",
        "    elif any(i in departure_head for i in head_1_string) and not any(i in departure_head for i in head_0_string) :\n",
        "        departure = list(d.keys())[1]\n",
        "        arrival = list(d.keys())[0]\n",
        "    else :\n",
        "        departure = list(d.keys())[0]\n",
        "        arrival = list(d.keys())[1]\n",
        "\n",
        "    return [departure, arrival]\n",
        "\n",
        "#extract methode\n",
        "\n",
        "def extract_cities_with_algo(sentence):\n",
        "\t\tif location_count(sentence):\n",
        "\t\t\tsentence = sentence\n",
        "\t\telse:\n",
        "\t\t\tprint(\"sentence is not valid\")\n",
        "\n",
        "\t\tdoc = nlp(sentence)\n",
        "\t\tdata = {}\n",
        "\n",
        "\t\tfor ent in doc.ents:\n",
        "\t\t\t\tif ent.label_ == \"LOC\":\n",
        "\t\t\t\t\t\tdata[f\"{ent.text}\"] = {\"label\":ent.label_}\n",
        "\t\t\t\t\t\tfor token in ent:\n",
        "\t\t\t\t\t\t\t\tdata[f\"{ent.text}\"][\"head\"] = token.head.text\n",
        "\t\t\t\t\t\t\t\tdata[f\"{ent.text}\"][\"children\"] = [child for child in token.children]\n",
        "\n",
        "\t\tprediction = predict(data)\n",
        "\t\tprint(\"Departure : \",prediction[0])\n",
        "\t\tprint(\"Destination : \",prediction[1])\n",
        "\t\treturn prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09YWlPRSbmDY"
      },
      "source": [
        "### 3.2 Custom NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twadiYNvxz23"
      },
      "source": [
        "### Train model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSklKBPbydUL",
        "outputId": "81cb94d5-0384-4609-8e6b-842d757dc1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Je voudrais aller à Paris en partant de Montpellier', {'entities': [(40, 51, 'DEPARTURE_LOC'), (20, 25, 'DESTINATION_LOC')]}), ('Est-ce que tu peux me trouver un itinéraire de Paris à Montpellier', {'entities': [(47, 52, 'DEPARTURE_LOC'), (55, 66, 'DESTINATION_LOC')]}), ('Je souhaite aller à Lyon en partant de Montpellier', {'entities': [(39, 50, 'DEPARTURE_LOC'), (20, 24, 'DESTINATION_LOC')]}), ('Je suis à Montpellier, je vous aller à Lyon', {'entities': [(10, 21, 'DEPARTURE_LOC'), (39, 43, 'DESTINATION_LOC')]}), ('Trouve moi un itinéraire pour Lyon Paris', {'entities': [(30, 34, 'DEPARTURE_LOC'), (35, 40, 'DESTINATION_LOC')]})]\n"
          ]
        }
      ],
      "source": [
        "training_data = []\n",
        "\n",
        "for index, row in excel_data.iterrows():\n",
        "    sentence = row['sentence']\n",
        "    departure = row['departure']\n",
        "    destination = row['destination']\n",
        "    training_data.append((row['sentence'], {\"entities\": [\n",
        "        (sentence.find(departure), sentence.find(departure) + len(departure), \"DEPARTURE_LOC\")\n",
        "        ,(sentence.find(destination), sentence.find(destination) + len(destination), \"DESTINATION_LOC\")\n",
        "    ]}))\n",
        "\n",
        "print(training_data[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLnAgLEkyowY",
        "outputId": "e8bc4736-e78b-476c-e047-41408b0f0cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "72\n",
            "18\n"
          ]
        }
      ],
      "source": [
        "# shuffle training_data\n",
        "import random\n",
        "\n",
        "random.shuffle(training_data)\n",
        "\n",
        "#split training data into train and test data sets\n",
        "print(len(training_data))\n",
        "\n",
        "test_data = training_data[:len(training_data)//5]\n",
        "train_data = training_data[len(training_data)//5:]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlLMQIxtyuBm",
        "outputId": "bec79cea-b521-45af-fe9a-fd4ad784a7e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:00<00:00, 2115.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:00<00:00, 1797.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping entity\n",
            "Skipping entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_md\")\n",
        "\n",
        "db_train = DocBin()\n",
        "db_test = DocBin()\n",
        "\n",
        "for text, annot in tqdm(train_data):\n",
        "    doc = nlp.make_doc(text)\n",
        "    ents = []\n",
        "    for start, end, label in annot[\"entities\"]:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is None:\n",
        "            print(\"Skipping entity\")\n",
        "        else:\n",
        "            ents.append(span)\n",
        "\n",
        "    doc.ents = ents\n",
        "    db_train.add(doc)\n",
        "\n",
        "\n",
        "\n",
        "for text, annot in tqdm(test_data):\n",
        "    doc = nlp.make_doc(text)\n",
        "    ents = []\n",
        "    for start, end, label in annot[\"entities\"]:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is None:\n",
        "            print(\"Skipping entity\")\n",
        "        else:\n",
        "            ents.append(span)\n",
        "    doc.ents = ents\n",
        "    db_train.add(doc)\n",
        "\n",
        "db_train.to_disk(\"./train.spacy\")\n",
        "db_test.to_disk(\"./test.spacy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBKNn9sozJUF",
        "outputId": "d8b2ebd1-d8e0-40a0-bd7e-8b8cd07bda5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('config.cfg', <http.client.HTTPMessage at 0x79fc1fcffe20>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# download spacy config\n",
        "urllib.request.urlretrieve(\"https://drive.google.com/uc?export=download&id=1NloGxCPXKFaLfMpwZANnOBi2taeZrIJy\", \"config.cfg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es1M9kzOyyJ5",
        "outputId": "7969be3d-d4f2-4c7d-9233-5640493b1bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-06 11:53:34.407746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-08-06 11:53:41,749] [INFO] Set up nlp object from config\n",
            "[2023-08-06 11:53:41,779] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2023-08-06 11:53:41,785] [INFO] Created vocabulary\n",
            "[2023-08-06 11:53:43,987] [INFO] Added vectors: fr_core_news_md\n",
            "[2023-08-06 11:53:43,988] [INFO] Finished initializing nlp object\n",
            "[2023-08-06 11:53:44,451] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     43.60    0.00    0.00    0.00    0.00\n",
            " 12     200         13.90    856.12    0.00    0.00    0.00    0.00\n",
            " 28     400          3.53     11.35    0.00    0.00    0.00    0.00\n",
            " 48     600          1.20      2.40    0.00    0.00    0.00    0.00\n",
            " 72     800          0.00      0.00    0.00    0.00    0.00    0.00\n",
            "101    1000          0.00      0.00    0.00    0.00    0.00    0.00\n",
            "137    1200          0.00      0.00    0.00    0.00    0.00    0.00\n",
            "181    1400          0.00      0.00    0.00    0.00    0.00    0.00\n",
            "232    1600          0.00      0.00    0.00    0.00    0.00    0.00\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./test.spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPylvAtkuRNU"
      },
      "outputs": [],
      "source": [
        "def extract_cities_with_custom_NER(sentence):\n",
        "\n",
        "  nlp_custom_ner = spacy.load(\"/content/output/model-best\")\n",
        "  doc = nlp_custom_ner(sentence)\n",
        "\n",
        "  departure= \"\"\n",
        "  destination = \"\"\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ == \"DEPARTURE_LOC\":\n",
        "      departure = ent.text\n",
        "    if ent.label_== \"DESTINATION_LOC\":\n",
        "      destination = ent.text\n",
        "\n",
        "  print(\"Departure : \", departure)\n",
        "  print(\"Destination : \", destination)\n",
        "\n",
        "  return departure, destination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GDXDW7hgTqM"
      },
      "source": [
        "### 3.3 Open AI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQA-yrfcyCxj",
        "outputId": "bce212f9-d4c7-4e64-99c3-75cbe5e04f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-rP0eFQyAsV"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "#construct prompt with context and example\n",
        "def generate_prompt(sentence):\n",
        "\treturn \"\"\"Trouve la gare de départ et la gare d'arrivée.\n",
        "\\nPhrase: Trouve moi le trajet le plus court entre Paris et Montpellier en passant par Lyon.\n",
        "\\nReponse:Paris;Montpellier\n",
        "\\nPhrase:{}\n",
        "\\nReponse:\"\"\".format(sentence)\n",
        "\n",
        "openai.api_key = \"\"\n",
        "#api call\n",
        "def openai_api_call(travel_request):\n",
        "\tcall_response = openai.Completion.create(\n",
        "\t\t\t\t\t\tmodel=\"text-davinci-003\",\n",
        "\t\t\t\t\t\tprompt=generate_prompt(travel_request),\n",
        "\t\t\t\t\t\ttemperature=0.7,\n",
        "\t\t\t\t\t\tmax_tokens=100)\n",
        "\treturn call_response.choices[0].text\n",
        "\n",
        "def format_response(response):\n",
        "\tsplitted_res = response.split(';')\n",
        "\treturn splitted_res\n",
        "\n",
        "def extract_cities_with_openai(travel_request):\n",
        "\tres = openai_api_call(travel_request)\n",
        "\tformatted_response = format_response(res)\n",
        "\tprint(\"Departure : \",formatted_response[0])\n",
        "\tprint(\"Destination : \",formatted_response[1])\n",
        "\treturn formatted_response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw2daO_NagVi"
      },
      "source": [
        "## Module 3: Pathfinding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cx0FyNXadl2"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import csv\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci9J1z2XaqiM"
      },
      "outputs": [],
      "source": [
        "class Graph():\n",
        "    def __init__(self):\n",
        "        self.edges = defaultdict(list)\n",
        "        self.weights = {}\n",
        "\n",
        "    def add_edge(self, from_node, to_node, weight):\n",
        "        # Note: assumes edges are bi-directional\n",
        "        self.edges[from_node].append(to_node)\n",
        "        self.edges[to_node].append(from_node)\n",
        "        self.weights[(from_node, to_node)] = weight\n",
        "        self.weights[(to_node, from_node)] = weight\n",
        "\n",
        "graph = Graph()\n",
        "urllib.request.urlretrieve(\"https://drive.google.com/uc?export=download&id=1CL_z0bH464x42fRfWwM1qJaGKjd9fCtp\", \"sncf_data.csv\")\n",
        "\n",
        "with open('/content/sncf_data.csv', newline='', encoding='UTF-8') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "\n",
        "    next(reader)\n",
        "\n",
        "    for row in reader:\n",
        "        graph.add_edge(row[1].lower(), row[2].lower(), int(row[3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjFUUuFibFs9"
      },
      "outputs": [],
      "source": [
        "def pathfinder(graph, initial, end):\n",
        "    initial = initial.lower()\n",
        "    end = end.lower()\n",
        "    # shortest paths is a dict of nodes\n",
        "    # whose value is a tuple of (previous node, weight)\n",
        "    shortest_paths = {initial: (None, 0)}\n",
        "    current_node = initial\n",
        "    visited = set()\n",
        "\n",
        "    while current_node != end:\n",
        "        visited.add(current_node)\n",
        "        destinations = graph.edges[current_node]\n",
        "        weight_to_current_node = shortest_paths[current_node][1]\n",
        "\n",
        "        for next_node in destinations:\n",
        "            weight = graph.weights[(current_node, next_node)] + weight_to_current_node\n",
        "            if next_node not in shortest_paths:\n",
        "                shortest_paths[next_node] = (current_node, weight)\n",
        "            else:\n",
        "                current_shortest_weight = shortest_paths[next_node][1]\n",
        "                if current_shortest_weight > weight:\n",
        "                    shortest_paths[next_node] = (current_node, weight)\n",
        "\n",
        "        next_destinations = {node: shortest_paths[node] for node in shortest_paths if node not in visited}\n",
        "        if not next_destinations:\n",
        "            return \"Route Not Possible\"\n",
        "        # next node is the destination with the lowest weight\n",
        "        current_node = min(next_destinations, key=lambda k: next_destinations[k][1])\n",
        "\n",
        "    # Work back through destinations in shortest path\n",
        "    path = []\n",
        "    while current_node is not None:\n",
        "        path.append(current_node)\n",
        "        next_node = shortest_paths[current_node][0]\n",
        "        current_node = next_node\n",
        "    # Reverse path\n",
        "    path = path[::-1]\n",
        "    return path, weight\n",
        "\n",
        "def cityToStation(graph, city):\n",
        "    list = []\n",
        "    for key, val in graph.edges.items():\n",
        "        if city in key:\n",
        "            list += [key]\n",
        "    return list\n",
        "\n",
        "def get_shortest_path(departure, arrival):\n",
        "    stationsDeparture = cityToStation(graph, departure)\n",
        "    stationsArrival = cityToStation(graph, arrival)\n",
        "    min_weight = 10000\n",
        "    final_path = 0\n",
        "    for stationD in stationsDeparture:\n",
        "        for stationA in stationsArrival:\n",
        "            path, weight = pathfinder(graph, stationD, stationA)\n",
        "            if weight < min_weight:\n",
        "                final_path = path\n",
        "                min_weight = weight\n",
        "\n",
        "    print(\"The shortest path is : \", final_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP_DVyNWoJY9"
      },
      "source": [
        "# DEMO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qxvd3W5AZH8"
      },
      "source": [
        "## 1 - Record audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "3pCryCZO_o8G",
        "outputId": "902c8568-6d3b-493a-c596-7bfa706d645e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "var my_div = document.createElement(\"DIV\");\n",
              "var my_p = document.createElement(\"P\");\n",
              "var my_btn = document.createElement(\"BUTTON\");\n",
              "var t = document.createTextNode(\"Press to start recording\");\n",
              "\n",
              "my_btn.appendChild(t);\n",
              "//my_p.appendChild(my_btn);\n",
              "my_div.appendChild(my_btn);\n",
              "document.body.appendChild(my_div);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "var recordButton = my_btn;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "  gumStream = stream;\n",
              "  var options = {\n",
              "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
              "    mimeType : 'audio/webm;codecs=opus'\n",
              "    //mimeType : 'audio/webm;codecs=pcm'\n",
              "  };            \n",
              "  //recorder = new MediaRecorder(stream, options);\n",
              "  recorder = new MediaRecorder(stream);\n",
              "  recorder.ondataavailable = function(e) {            \n",
              "    var url = URL.createObjectURL(e.data);\n",
              "    var preview = document.createElement('audio');\n",
              "    preview.controls = true;\n",
              "    preview.src = url;\n",
              "    document.body.appendChild(preview);\n",
              "\n",
              "    reader = new FileReader();\n",
              "    reader.readAsDataURL(e.data); \n",
              "    reader.onloadend = function() {\n",
              "      base64data = reader.result;\n",
              "      //console.log(\"Inside FileReader:\" + base64data);\n",
              "    }\n",
              "  };\n",
              "  recorder.start();\n",
              "  };\n",
              "\n",
              "recordButton.innerText = \"Recording... press to stop\";\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "\n",
              "function toggleRecording() {\n",
              "  if (recorder && recorder.state == \"recording\") {\n",
              "      recorder.stop();\n",
              "      gumStream.getAudioTracks()[0].stop();\n",
              "      recordButton.innerText = \"\"\n",
              "  }\n",
              "}\n",
              "\n",
              "// https://stackoverflow.com/a/951057\n",
              "function sleep(ms) {\n",
              "  return new Promise(resolve => setTimeout(resolve, ms));\n",
              "}\n",
              "\n",
              "var data = new Promise(resolve=>{\n",
              "//recordButton.addEventListener(\"click\", toggleRecording);\n",
              "recordButton.onclick = ()=>{\n",
              "toggleRecording()\n",
              "\n",
              "sleep(2000).then(() => {\n",
              "  // wait 2000ms for the data to be available...\n",
              "  // ideally this should use something like await...\n",
              "  //console.log(\"Inside data:\" + base64data)\n",
              "  resolve(base64data.toString())\n",
              "\n",
              "});\n",
              "\n",
              "}\n",
              "});\n",
              "      \n",
              "</script>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "audio_data = get_audio()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWku-HebAdHo"
      },
      "source": [
        "## 2 - Get travel request using speech to text api from google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-In80FAAVLb",
        "outputId": "9b1bbe00-dbbd-4fe2-dbd1-2200659a35af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The travel request sentence is :  salut je suis une fille je suis actuellement à Paris et j'aimerais bien aller à Toulouse\n"
          ]
        }
      ],
      "source": [
        "travel_request = speech_to_text(audio_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pitJuJ5bCbHE"
      },
      "source": [
        "## 3 - Extract departure and destination cities from the travel request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6uge0mibtBd"
      },
      "source": [
        "### 3.1 - Algorithmic approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UB9mOY5COr_",
        "outputId": "382be8cc-622d-417d-e900-024b24a53c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Departure :  Paris\n",
            "Destination :  Toulouse\n"
          ]
        }
      ],
      "source": [
        "departure, destination = extract_cities_with_algo(travel_request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keqqBw3zFulR"
      },
      "source": [
        "### 3.2 Custom trained NER approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "730WbFh1Ftl9",
        "outputId": "234a1a4a-7958-442e-af45-5c0442d67645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Departure :  Paris\n",
            "Destination :  Toulouse\n"
          ]
        }
      ],
      "source": [
        "departure, destination = extract_cities_with_custom_NER(travel_request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4--uhFqwnCh"
      },
      "source": [
        "### 3.3 OpenAI api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwTvyWsyw16i",
        "outputId": "cb158771-3cf5-4beb-b85a-b2cf9327a852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Departure :  Paris\n",
            "Destination :  Toulouse\n"
          ]
        }
      ],
      "source": [
        "departure, destination = extract_cities_with_openai(travel_request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJT9N4OfZIgB"
      },
      "source": [
        "## 4 - Pathfinding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjE-bUX1ZGot",
        "outputId": "0bec2177-198d-42c8-d645-232b6cc6fcdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shortest path is :  ['paris-austerlitz', 'vierzon', 'limoges-bénédictins', 'agen', 'toulouse-matabiau']\n"
          ]
        }
      ],
      "source": [
        "get_shortest_path(departure.lower(), destination.lower())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}